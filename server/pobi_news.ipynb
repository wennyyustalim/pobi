{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG tool that adds transparency to political bias in top stories, by site"
      ],
      "metadata": {
        "id": "Y2m2l-vt_RSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Installing libraries and adding keys"
      ],
      "metadata": {
        "id": "bKZWdc1_J5hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "p7eldKitzkWl"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pillow==10.0.1\n",
        "!pip install -q unstructured-client unstructured[all-docs]==0.12.5 langchain transformers accelerate bitsandbytes sentence-transformers faiss-gpu"
      ],
      "metadata": {
        "id": "rJ9juQ-XKJKK"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install poppler-utils tesseract-ocr"
      ],
      "metadata": {
        "id": "eSGREYus_Lyh"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "id": "jMyeU-w_z9a9"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "D7vB0DQj0evs"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG0dXhBt4MzQ",
        "outputId": "39b01364-65d2-44d3-f3c8-ca44b0efdff5"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "3JWGSEoZKbtN"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "unstructered_api_key = getpass('Enter Unstructured API Key')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwBf16RjAdXj",
        "outputId": "1ef80f36-910f-4af7-c073-4efdb1ed9fef"
      },
      "execution_count": 139,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Unstructured API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# taking this from above for unstructured\n",
        "os.environ[\"UNSTRUCTURED_API_KEY\"] = unstructered_api_key"
      ],
      "metadata": {
        "id": "KDHdcW4I8bgb"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup client\n",
        "from unstructured_client import UnstructuredClient\n",
        "\n",
        "unstructured_api_key = os.environ.get(\"UNSTRUCTURED_API_KEY\")\n",
        "\n",
        "client = UnstructuredClient(\n",
        "    api_key_auth=unstructured_api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "OIG6V3swKyIZ"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter rest of keys\n",
        "serper_key = getpass('Enter Serper API Key')\n",
        "hugging_face_api_key = getpass('Enter HuggingFace API Key')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIIOR5hcBM-M",
        "outputId": "1daad7de-075e-4d09-d4a2-4c9f08fccf63"
      },
      "execution_count": 144,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Serper API Key··········\n",
            "Enter HuggingFace API Key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign keys\n",
        "os.environ[\"SERPER_KEY\"] = serper_key\n",
        "os.environ[\"HF_API_KEY\"] = hugging_face_api_key"
      ],
      "metadata": {
        "id": "y_t0D3abBeat"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SERPER_KEY = os.environ[\"SERPER_KEY\"]\n",
        "HF_API_KEY = os.environ[\"HF_API_KEY\"]"
      ],
      "metadata": {
        "id": "wFtSpw0EFqw3"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do the same for OAI API key\n",
        "oai_key = getpass('Enter OAI key')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD7ZPF6_GQSx",
        "outputId": "0e38afe0-1684-4b1b-c854-5e5e538e7d9c"
      },
      "execution_count": 147,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OAI key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "from unstructured_client.models import shared\n",
        "from unstructured_client.models.errors import SDKError\n",
        "from unstructured.staging.base import dict_to_elements\n",
        "from langchain_core.documents import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain import OpenAI, VectorDBQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader, JSONLoader, UnstructuredFileLoader, WebBaseLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain, SequentialChain, RetrievalQA\n",
        "from langchain.memory import VectorStoreRetrieverMemory\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.utilities import GoogleSerperAPIWrapper\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document, StrOutputParser\n",
        "from bs4 import BeautifulSoup\n",
        "import faiss\n",
        "import json\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "import requests\n",
        "from unstructured.cleaners.core import clean_extra_whitespace\n",
        "from langchain_community.document_loaders import UnstructuredPDFLoader"
      ],
      "metadata": {
        "id": "bx4VIu6AoZRk"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF for RAG to define liberal and conservative text in language\n",
        "path_to_pdf=(\"/content/Flanker paper.PolPsychVersion.04-27-11.pdf\")\n",
        "\n",
        "with open(path_to_pdf, \"rb\") as f:\n",
        "  files=shared.Files(\n",
        "      content=f.read(),\n",
        "      file_name=path_to_pdf,\n",
        "      )\n",
        "  req = shared.PartitionParameters(\n",
        "    files=files,\n",
        "    chunking_strategy=\"by_title\",\n",
        "    max_characters=512,\n",
        "  )\n",
        "  try:\n",
        "    resp = client.general.partition(req)\n",
        "  except SDKError as e:\n",
        "    print(e)\n",
        "\n",
        "elements = dict_to_elements(resp.elements)"
      ],
      "metadata": {
        "id": "E6TiAW2EoBpO"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "for element in elements:\n",
        "    metadata = element.metadata.to_dict()\n",
        "    documents.append(Document(page_content=element.text, metadata=metadata))\n",
        "\n",
        "db = FAISS.from_documents(documents, OpenAIEmbeddings(openai_api_key=oai_key))\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
      ],
      "metadata": {
        "id": "q_1rPCCy3Czf"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define variables - from UI so placholders\n",
        "search_query = 'BBC'\n",
        "#num_results = 5\n",
        "personal_political_assessment = \"5\""
      ],
      "metadata": {
        "id": "8Q8iH-6uI-Do"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo', max_tokens=1024, openai_api_key=oai_key)\n",
        "search = GoogleSerperAPIWrapper(type=\"news\", tbs=\"qdr:w1\", serper_api_key=SERPER_KEY)\n",
        "result_dict = search.results(search_query)\n",
        "token_limit=8000"
      ],
      "metadata": {
        "id": "oDMDIRufEvck"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in zip(range(num_results), result_dict['news']):\n",
        "    url = item.get('link','N/A')\n",
        "    loader = WebBaseLoader(url)\n",
        "    article = loader.load()"
      ],
      "metadata": {
        "id": "tX7WD9wTJFet"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])"
      ],
      "metadata": {
        "id": "4An38C1e6ID0"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First Chain - assess the article in a chain for langchain\n",
        "first_template = ChatPromptTemplate.from_template(\"\"\"You are a university political science professor who uses brevity and inherent logic to assess any political bias from language in a news article {article}. Summarize the article in as few words as possible, being extremely clear. Follow the steps below to output an analysis in one brief sentence. Then list For Potential Logical Fallacies, return a short numbered list of logical fallacies as defined by Aristotle ranked in order of impact on society and ability to mislead, explain why it is a logical fallacy briefly. Do not list any more than 3. If no fallacies are present, return a brief note that no major logical fallacies are found.\n",
        "Return output using the labels below, with a new line of text directly under each label.\\\\\n",
        "and rate it on a scale of between 1 and 10, with 1 being extremely conservative and 10 being extremely liberal. \\\n",
        "After assessing the article, normalize this in relation to a defined personal political assessment with value {personal_political_assessment} for an audience defines their own political beliefs, on a scale of 1 to 10 with 1 being extremely conservative and 10 being extremely liberal, and explain the strongest reason for the score and its scale in a brief sentence \\\n",
        "So for example if the user is defined as a 5 then no difference should be inherent in the assessment of the political bias from the news article.  \\\n",
        "If the user is defined as a 3 then the political bias assessed in the news article should be increased by a factor of 2. \\\n",
        "If a user is defined as a 7 then the political bias assessed in the news article should be decreased by a factor of 2. \\\n",
        "Do not explain anything in the first person, just assess in one sentence based on the values and return the result and explain the reason for the political bias assessment.\n",
        "Summary:\n",
        "Potential Political Bias Assessment on a scale of 1 to 10:\n",
        "Reason for Political Bias:\n",
        "Potential Logical Fallacies:\n",
        "\"\"\")\n",
        "#prompt_template1 = PromptTemplate(input_variables=[\"article\", \"personal_political_assessment\"], template=first_template)\n",
        "first_chain = first_template | llm | StrOutputParser()\n",
        "#first_chain = LLMChain(llm=llm, prompt=first_chain, output_key='Assessment')"
      ],
      "metadata": {
        "id": "a9O_f7a2baos"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = first_chain.invoke({\"article\": article, \"personal_political_assessment\": personal_political_assessment})\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADTe8_bOfHNd",
        "outputId": "370e5d50-eb4f-4b6d-9d3c-25e8b9560b52"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: The article discusses how atoll islands like the Maldives have not shrunk despite rising sea levels, offering hope for their future.\n",
            "\n",
            "Potential Political Bias Assessment on a scale of 1 to 10: 3\n",
            "Reason for Political Bias: The article focuses on scientific research and environmental issues without displaying any overt political bias.\n",
            "\n",
            "Potential Logical Fallacies: \n",
            "1. False Cause - implying that rising sea levels are not a threat to atoll islands based on limited data.\n",
            "2. Hasty Generalization - drawing conclusions about all atoll islands based on a study of a limited number of islands.\n",
            "3. Appeal to Authority - relying heavily on the opinions of scientists without considering alternative perspectives.\n"
          ]
        }
      ]
    }
  ]
}